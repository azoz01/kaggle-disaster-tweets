{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from preprocessing.preparing import clean_data\n",
    "from preprocessing.lemmatization import lemmatize\n",
    "from preprocessing.stemming import stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     Our Deeds are the Reason of this #earthquake M...\n",
       "1                Forest fire near La Ronge Sask. Canada\n",
       "2     All residents asked to 'shelter in place' are ...\n",
       "3     13,000 people receive #wildfires evacuation or...\n",
       "4     Just got sent this photo from Ruby #Alaska as ...\n",
       "...                                                 ...\n",
       "7608  Two giant cranes holding a bridge collapse int...\n",
       "7609  @aria_ahrary @TheTawniest The out of control w...\n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
       "7611  Police investigating after an e-bike collided ...\n",
       "7612  The Latest: More Homes Razed by Northern Calif...\n",
       "\n",
       "[7613 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "target = df[['target']]\n",
    "text_col = df[['text']]\n",
    "text_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full cleaning, stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 'To lower'\n",
      "100%|██████████| 7613/7613 [00:00<00:00, 142372.20it/s]\n",
      "ic| 'Remove mail'\n",
      "100%|██████████| 7613/7613 [00:00<00:00, 68256.12it/s]\n",
      "ic| 'Remove urls'\n",
      "100%|██████████| 7613/7613 [00:00<00:00, 128624.29it/s]\n",
      "ic| 'Remove Twitter references'\n",
      "100%|██████████| 7613/7613 [00:00<00:00, 46515.87it/s]\n",
      "ic| 'Remove punctuation'\n",
      "100%|██████████| 7613/7613 [00:00<00:00, 57057.15it/s]\n",
      "ic| 'Remove numbers'\n",
      "100%|██████████| 7613/7613 [00:00<00:00, 109370.78it/s]\n",
      "ic| 'Remove stopwords'\n",
      "100%|██████████| 7613/7613 [01:19<00:00, 95.92it/s] \n"
     ]
    }
   ],
   "source": [
    "clean = clean_data(text_col, correct_spelling = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing entries\n",
    "target = target.iloc[np.logical_not(clean.isna().values)]\n",
    "clean = clean.iloc[np.logical_not(clean.isna().values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.to_csv('prepared/target.csv', index=False)\n",
    "clean.to_csv('prepared/clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            deeds reason earthquake may allah forgive us\n",
       "1                    forest fire near la range ask canada\n",
       "2       residents asked shelter place notified officer...\n",
       "3       people receive wildfires evacuation orders cal...\n",
       "4       got sent photo ruby alaska smoke wildfires hou...\n",
       "                              ...                        \n",
       "7607    two giant cranks holding bridge collapse nearb...\n",
       "7608    array control wild fires california even north...\n",
       "7609                                  tuck volcano hawaii\n",
       "7610    police investigation bike collided car little ...\n",
       "7611    latest homes razed northern california wildlif...\n",
       "Name: text, Length: 7612, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = lemmatize(clean)\n",
    "lemmatized.to_csv('prepared/lemmatized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               deed reason earthquak may allah forgiv us\n",
       "1                     forest fire near la rang ask canada\n",
       "2       resid ask shelter place notifi offic evacu she...\n",
       "3             peopl receiv wildfir evacu order california\n",
       "4       got sent photo rubi alaska smoke wildfir hour ...\n",
       "                              ...                        \n",
       "7607       two giant crank hold bridg collaps nearbi home\n",
       "7608    array control wild fire california even northe...\n",
       "7609                                  tuck volcano hawaii\n",
       "7610    polic investig bike collid car littl portug bi...\n",
       "7611    latest home raze northern california wildlif a...\n",
       "Name: text, Length: 7612, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed = stem(clean)\n",
    "stemmed.to_csv('prepared/stemmed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning without stopwords removal and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean_data(text_col, remove_stopwords=False)\n",
    "clean.to_csv('prepared/clean_with_stopwords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize(clean).to_csv('prepared/lemmatized_with_stopwords.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca5500626be4521731e94377c4d4cb7f418bbb2f8316b97dc0f44c020a71d1f4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('Sandbox': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
